{
    "version": "0",
    "model_settings": {
        "providers": {
            "OpenAI": {
                "models": {
                    "gpt-4o": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 128000,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Points to the latest version of the gpt-4o (Omni) model. OpenAI's high-intelligence flagship model for complex, multi-step tasks. GPT-4o is cheaper and faster than GPT-4 Turbo."
                    },
                    "gpt-4o-mini": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 128000,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Points to the latest version of the gpt-4o-mini. A lower-cost, intelligent, small model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo."
                    },
                    "gpt-4-turbo-preview": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 128000,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Points to the latest preview version of the gpt-4 preview model."
                    },
                    "gpt-4-0125-preview": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 128000,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Intended to reduce cases of “laziness” where the model doesn’t complete a task. Returns a maximum of 4,096 output tokens."
                    },
                    "gpt-4-1106-preview": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 128000,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Training data up to Apr 2023. Improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens."
                    },
                    "gpt-4": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 8192,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Training data up to Sep 2021"
                    },
                    "gpt-3.5-turbo": {
                        "temperature": 0.7,
                        "topP": 1,
                        "frequencyPenalty": 0,
                        "presencePenalty": 0,
                        "contextTokenSize": 4097,
                        "systemMessage": "You are a helpful advisor.",
                        "notes": "Training data up to Sep 2021"
                    }
                },
                "default": "gpt-4o",
                "releaseUpdates": {
                    "modelUpdate": {
                        "message": "Your default AI model has been set to gpt-4o, which is OpenAI's high-intelligence flagship model for complex, multi-step tasks. It's faster and lower cost than gpt4-turbo. Other models are available in Model Settings. For example, gpt-4o-mini is even faster and suitable for most simple tasks.",
                        "model": "gpt-4o",
                        "pending": true
                    }
                }
            }
        },
        "default": "OpenAI"
    },
    "sliders": {
        "temperature": {
            "marks": [
                { "value": 0, "label": "0" },
                { "value": 0.5, "label": "0.5" },
                { "value": 1, "label": "1" },
                { "value": 1.5, "label": "1.5" },
                { "value": 2, "label": "2" }
            ],
            "min": 0,
            "max": 2,
            "step": 0.1,
            "default": 0.7,
            "caption": "Between 0 and 2. 0 is the most consistent 'best' prediction. Up to 0.9 allows more creativity and variability. Above 1, the model is more 'creative' but can hallucinate more and make spelling and grammar mistakes."
        },
        "top_p": {
            "marks": [
                { "value": 0, "label": "0" },
                { "value": 0.25, "label": "0.25" },
                { "value": 0.5, "label": "0.5" },
                { "value": 0.75, "label": "0.75" },
                { "value": 1, "label": "1" }
            ],
            "min": 0,
            "max": 1,
            "step": 0.1,
            "default": 1,
            "caption": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
        },
        "presence_penalty": {
            "marks": [
                { "value": -2, "label": "2" },
                { "value": -1, "label": "-1" },
                { "value": 0, "label": "0" },
                { "value": 1, "label": "1" },
                { "value": 2, "label": "2" }
            ],
            "min": -2,
            "max": 2,
            "step": 0.1,
            "default": 0,
            "caption": "Positive values penalise new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
        },
        "frequency_penalty": {
            "label": "frequency_penalty",
            "marks": [
                { "value": -2, "label": "2" },
                { "value": -1, "label": "-1" },
                { "value": 0, "label": "0" },
                { "value": 1, "label": "1" },
                { "value": 2, "label": "2" }
            ],
            "min": -2,
            "max": 2,
            "step": 0.1,
            "default": 0,
            "caption": "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
        }
    }
}